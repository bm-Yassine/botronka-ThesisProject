- Local “AI Agent” stack (Pi-friendly, offline)

STT (Speech-to-Text): whisper.cpp
Fast C/C++ inference, runs well on ARM.
Official repo shows whisper-cli usage and quick demo (make base.en) and CLI help.
Model download options are documented in models/README.

LLM (the “agent brain”): llama.cpp server
Run a small GGUF model locally and talk to it over HTTP.
llama.cpp server supports OpenAI-compatible style API endpoints (good for clean integration).
On Pi 5, CPU+BLAS is the reliable baseline; Vulkan offload can be hit-or-miss.
Good model size for Pi 5 (8GB): Qwen2.5 1.5B Instruct (Q4) (GGUF available).

TTS (Text-to-Speech): piper-tts
Piper is now maintained under OHF-Voice/piper1-gpl.
Install is straightforward via pip.
Voice models are typically a pair: *.onnx + matching *.onnx.json.

VAD (end-of-speech detection): webrtcvad
Lightweight and ideal for cutting the mic automatically.
Requires 16-bit mono PCM, sample rates {8k,16k,32k,48k}, and frame sizes 10/20/30ms.

- High level flow:
1. Vision thread sets:
face_present=True/False
trust_level (0 unknown: answers "who are you?", 1 known : can answer, 2+ : allowed to move, 3 owner : can promote "name" to "trust level")
2. When face_present flips to True:
agent: greet once (TTS)
3. While face present:
VAD segments utterance
STT transcribes
Agent outputs JSON {type, speak, command, requires_trust}
4. Policy checks trust + safety
Execute or refuse + explain via TTS
Face disappears for 4s or timeout → stop mic and go IDLE